# Configuration du crawler web

[crawler]
# URL de départ
start_url = "https://wikipedia.com"

# Nombre de threads pour le crawling
threads = 8

# Délai entre les requêtes (en millisecondes)
delay_ms = 30

# User-Agent
user_agent = "Mozilla/5.0 (Windows NT 5.1; rv:5.0.1) Gecko/20100101 Firefox/5.0.1"

# Timeout pour les requêtes (en secondes)
timeout = 30

# Profondeur maximale (0 = illimité)
max_depth = 0

# Domaine spécifique (laisser vide pour crawler tous les domaines)
specific_domain = ""

[patterns]
# Patterns regex à rechercher (laisser vide pour désactiver)
# Exemples:
# email_pattern = "[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}"
# phone_pattern = "\\+?[0-9]{1,3}[-. ]?\\(?[0-9]{1,4}\\)?[-. ]?[0-9]{1,4}[-. ]?[0-9]{1,9}"
patterns = [
    "[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}",
    "(?:0[1-9](?:[0-9]{8})|(?:\+33|0033)[1-9](?:[0-9]{8}))"
]

[database]
# Chemin vers la base de données SQLite
path = "crawler.db"

[logging]
# Niveau de log: INFO, WARN, ERROR
level = "INFO"
# Fichier de log
file = "crawler.log"
